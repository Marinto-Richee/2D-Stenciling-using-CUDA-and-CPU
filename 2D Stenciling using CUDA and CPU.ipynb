{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import cuda, njit\n",
    "import cpuinfo\n",
    "import GPUtil\n",
    "\n",
    "# Function to get GPU information\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        gpu_info = GPUtil.getGPUs()[0]  # Assuming there is at least one GPU\n",
    "        return gpu_info\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu with array size 2000 x 2000\n",
      "Running on cuda with array size 2000 x 2000\n",
      "The GPU speedup factor is approximately 1.39 times.\n",
      "-------------------------------------------\n",
      "Running on cpu with array size 4000 x 4000\n",
      "Running on cuda with array size 4000 x 4000\n",
      "The GPU speedup factor is approximately 18.18 times.\n",
      "-------------------------------------------\n",
      "Running on cpu with array size 8000 x 8000\n",
      "Running on cuda with array size 8000 x 8000\n",
      "The GPU speedup factor is approximately 15.14 times.\n",
      "-------------------------------------------\n",
      "Running on cpu with array size 10000 x 10000\n",
      "Running on cuda with array size 10000 x 10000\n",
      "The GPU speedup factor is approximately 9.70 times.\n",
      "-------------------------------------------\n",
      "Running on cpu with array size 12000 x 12000\n",
      "Running on cuda with array size 12000 x 12000\n",
      "The GPU speedup factor is approximately 9.16 times.\n",
      "-------------------------------------------\n",
      "Running on cpu with array size 15000 x 15000\n",
      "Running on cuda with array size 15000 x 15000\n",
      "The GPU speedup factor is approximately 11.29 times.\n",
      "-------------------------------------------\n",
      "Running on cpu with array size 16000 x 16000\n",
      "Running on cuda with array size 16000 x 16000\n",
      "The GPU speedup factor is approximately 9.15 times.\n",
      "-------------------------------------------\n",
      "   Device  Execution Time  Memory Usage    Throughput  Array Size\n",
      "0     cpu      969.571352     30.517578  2.058644e+04        2000\n",
      "1    cuda      698.348511     30.517578  2.858175e+04        2000\n",
      "2     cpu       96.585274    122.070312  8.274555e+05        4000\n",
      "3    cuda        5.314112    122.070312  1.503920e+07        4000\n",
      "4     cpu      297.977209    488.281250  1.073371e+06        8000\n",
      "5    cuda       19.684544    488.281250  1.624828e+07        8000\n",
      "6     cpu      388.156176    762.939453  1.287626e+06       10000\n",
      "7    cuda       40.032703    762.939453  1.248479e+07       10000\n",
      "8     cpu      524.983883   1098.632812  1.371014e+06       12000\n",
      "9    cuda       57.283936   1098.632812  1.256478e+07       12000\n",
      "10    cpu      700.855970   1716.613770  1.604752e+06       15000\n",
      "11   cuda       62.065216   1716.613770  1.812126e+07       15000\n",
      "12    cpu      930.694580   1953.125000  1.374973e+06       16000\n",
      "13   cuda      101.695869   1953.125000  1.258340e+07       16000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_details = input(\"Do you want to print the details? (y/n): \")\n",
    "print_details = print_details.lower()\n",
    "#handle invalid input\n",
    "while print_details != 'y' and print_details != 'n':\n",
    "    print_details = input(\"Invalid input. Do you want to print the details? (y/n): \")\n",
    "    print_details = print_details.lower()\n",
    "@njit\n",
    "def laplacian_2d_cpu(a):\n",
    "    result = np.empty_like(a)\n",
    "    for i in range(1, a.shape[0] - 1):\n",
    "        for j in range(1, a.shape[1] - 1):\n",
    "            result[i, j] = -4 * a[i, j] + a[i - 1, j] + a[i + 1, j] + a[i, j - 1] + a[i, j + 1]\n",
    "    return result\n",
    "\n",
    "@cuda.jit\n",
    "def laplacian_2d_kernel(d_a):\n",
    "    i, j = cuda.grid(2)\n",
    "    \n",
    "    if 1 <= i < d_a.shape[0] - 1 and 1 <= j < d_a.shape[1] - 1:\n",
    "        laplacian_sum = -4 * d_a[i, j]\n",
    "        laplacian_sum += d_a[i - 1, j] + d_a[i + 1, j] + d_a[i, j - 1] + d_a[i, j + 1]\n",
    "        d_a[i, j] = laplacian_sum\n",
    "\n",
    "def measure_memory_usage(obj):\n",
    "    return obj.nbytes / (1024 ** 2)  # Convert bytes to megabytes\n",
    "\n",
    "def run_benchmark(device, array_size):\n",
    "    print(f\"Running on {device} with array size {array_size} x {array_size}\")\n",
    "    a = np.random.rand(array_size, array_size)\n",
    "    if device == 'cpu':\n",
    "        # Run on CPU\n",
    "        start_time_cpu = time.time()\n",
    "        result_cpu = laplacian_2d_cpu(a)\n",
    "        end_time_cpu = time.time()\n",
    "        execution_time_cpu = (end_time_cpu - start_time_cpu) * 1000  # Convert to milliseconds\n",
    "        memory_usage_cpu = measure_memory_usage(result_cpu)\n",
    "        ops = 5 * (array_size - 2) ** 2  # 5 operations per element, excluding boundary\n",
    "        throughput_cpu = ops / execution_time_cpu if execution_time_cpu > 1e-3 else 0  # OPS per second\n",
    "        # Memory usage on CPU\n",
    "        memory_usage_cpu = measure_memory_usage(result_cpu)\n",
    "        interior_cpu = result_cpu[1:-1, 1:-1]\n",
    "        del result_cpu\n",
    "        del a\n",
    "        if print_details == 'y':\n",
    "            print(f\"\\nCPU Throughput: {throughput_cpu:.2f} OPS\")\n",
    "            print(f\"CPU Execution Time: {execution_time_cpu:.5f} milliseconds\")\n",
    "            print(f\"Memory Usage (CPU): {memory_usage_cpu:.2f} MB\")\n",
    "        \n",
    "        return {'Device': device, 'Execution Time': execution_time_cpu, 'Memory Usage': memory_usage_cpu, 'Throughput': throughput_cpu}\n",
    "\n",
    "    elif device == 'cuda':\n",
    "        # Run on GPU\n",
    "        d_a = cuda.to_device(a)\n",
    "        threads_per_block = (16, 16)\n",
    "        blocks_per_grid = ((a.shape[0] - 1) // threads_per_block[0] + 1, (a.shape[1] - 1) // threads_per_block[1] + 1)\n",
    "        # Use CUDA events for accurate GPU timing\n",
    "        start_event = cuda.event()\n",
    "        end_event = cuda.event()\n",
    "        start_event.record()\n",
    "        laplacian_2d_kernel[blocks_per_grid, threads_per_block](d_a)\n",
    "        end_event.record()\n",
    "        # Synchronize and calculate elapsed time\n",
    "        end_event.synchronize()\n",
    "        execution_time_gpu = cuda.event_elapsed_time(start_event, end_event)# Convert to milliseconds\n",
    "        memory_usage_gpu = measure_memory_usage(d_a.copy_to_host())\n",
    "        ops = 5 * (array_size - 2) ** 2  # 5 operations per element, excluding boundary\n",
    "        min_execution_time_threshold = 1e-3  # Minimum threshold for execution time (adjust as needed)\n",
    "        throughput_gpu = ops / max(execution_time_gpu, min_execution_time_threshold)  # OPS per second\n",
    "        memory_usage_gpu = measure_memory_usage(d_a.copy_to_host())\n",
    "        interior_gpu = d_a.copy_to_host()[1:-1, 1:-1]\n",
    "        del d_a\n",
    "        del a\n",
    "        if print_details == 'y':\n",
    "            print(f\"\\nGPU Throughput: {throughput_gpu:.2f} OPS\")\n",
    "            print(f\"GPU Execution Time: {execution_time_gpu:.5f} milliseconds\")\n",
    "            print(f\"Memory Usage (GPU): {memory_usage_gpu:.2f} MB\")\n",
    "        return {'Device': device, 'Execution Time': execution_time_gpu, 'Memory Usage': memory_usage_gpu, 'Throughput': throughput_gpu}\n",
    "\n",
    "    # Compare results between CPU and GPU\n",
    "        \n",
    "    # Ensure the shapes match for comparison\n",
    "    min_shape = min(interior_cpu.shape[0], interior_gpu.shape[0])\n",
    "    interior_cpu = interior_cpu[:min_shape, :min_shape]\n",
    "    interior_gpu = interior_gpu[:min_shape, :min_shape]\n",
    "    threshold = np.max(np.abs(interior_cpu - interior_gpu))\n",
    "    match = np.allclose(interior_cpu, interior_gpu, atol=threshold)\n",
    "    print(f\"The interior elements match within the specified threshold of {threshold}. Match: {match} for {device}\")\n",
    "\n",
    "\n",
    "# Vary the array size for different runs\n",
    "array_sizes = [2000, 4000, 8000,10000,12000,15000, 16000]\n",
    "\n",
    "# List to store results\n",
    "results_list = []\n",
    "\n",
    "# Specify devices for comparison\n",
    "devices = ['cpu', 'cuda']\n",
    "\n",
    "for array_size in array_sizes:\n",
    "    for device in devices:\n",
    "        results = run_benchmark(device, array_size)\n",
    "        results['Array Size'] = array_size\n",
    "        results_list.append(results)\n",
    "    # Calculate speedup factor based on all readings\n",
    "    cpu_execution_times = results_list[-2]['Execution Time']\n",
    "    gpu_execution_times = results_list[-1]['Execution Time']\n",
    "\n",
    "    if gpu_execution_times > 0:\n",
    "        speedup_factors = cpu_execution_times / gpu_execution_times\n",
    "        print(f\"The GPU speedup factor is approximately {speedup_factors:.2f} times.\")\n",
    "    else:\n",
    "        print(\"The GPU execution times are very close to zero. Average speedup factor not defined.\")\n",
    "    print(\"-------------------------------------------\")\n",
    "\n",
    "# Create DataFrame from the list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n",
    "gpus = GPUtil.getGPUs()\n",
    "gpu_name = gpus[0].name\n",
    "#save resuls and plot in GPU name\n",
    "output_folder = gpu_name\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "results_df.to_csv(os.path.join(output_folder, f'{gpu_name}.csv'), index=False)\n",
    "\n",
    "# Save CPU and GPU information to a text file\n",
    "with open(os.path.join(output_folder,'system_info.txt'), 'w') as f:\n",
    "    # CPU information\n",
    "    f.write(\"CPU name: \" + cpuinfo.get_cpu_info().get('brand_raw') + \"\\n\")\n",
    "    f.write(\"CPU cores: \" + str(cpuinfo.get_cpu_info().get('count')) + \"\\n\")\n",
    "    f.write(\"CPU bits: \" + str(cpuinfo.get_cpu_info().get('bits')) + \"\\n\")\n",
    "    f.write(\"CPU frequency: \" + cpuinfo.get_cpu_info().get('hz_actual_friendly') + \"\\n\")\n",
    "    f.write(\"CPU architecture: \" + cpuinfo.get_cpu_info().get('arch') + \"\\n\")\n",
    "    f.write(\"CPU L2 Cache Size: \" + str(cpuinfo.get_cpu_info().get('l2_cache_size')) + \"\\n\")\n",
    "    f.write(\"CPU L3 Cache Size: \" + str(cpuinfo.get_cpu_info().get('l3_cache_size')) + \"\\n\\n\")\n",
    "\n",
    "    # GPU information\n",
    "    gpu_info = get_gpu_info()\n",
    "    if not isinstance(gpu_info, str):\n",
    "        f.write(\"GPU name: \" + gpu_info.name + \"\\n\")\n",
    "        f.write(\"GPU memory total: {:.2f} GB\\n\".format(gpu_info.memoryTotal/1024))\n",
    "        f.write(\"GPU memory used: {:.2f} GB\\n\".format(gpu_info.memoryUsed/1024))\n",
    "        f.write(\"GPU memory free: {:.2f} GB\\n\".format(gpu_info.memoryFree/1024))\n",
    "        f.write(\"GPU utilization: {:.2%}\\n\".format(gpu_info.load))\n",
    "    else:\n",
    "        f.write(\"GPU information not available.\\n\")\n",
    "\n",
    "\n",
    "# Your existing data and results_df\n",
    "\n",
    "# Function to save a plot as an image\n",
    "def save_plot_as_image(plt, file_name):\n",
    "    plt.savefig(os.path.join(output_folder,file_name) , bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Line chart for Execution Time with logarithmic scale\n",
    "plt.subplot(5, 1, 1)\n",
    "for device in devices:\n",
    "    device_results = results_df[results_df['Device'] == device]\n",
    "    plt.plot(device_results['Array Size'], device_results['Execution Time'], marker='o', label=device)\n",
    "plt.title('Execution Time Comparison')\n",
    "plt.xlabel('Array Size')\n",
    "plt.ylabel('Execution Time (milliseconds)')\n",
    "plt.legend()\n",
    "save_plot_as_image(plt, 'execution_time_comparison.png')\n",
    "\n",
    "# Line chart for Execution Time with logarithmic scale\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(5, 1, 2)\n",
    "for device in devices:\n",
    "    device_results = results_df[results_df['Device'] == device]\n",
    "    plt.plot(device_results['Array Size'], device_results['Execution Time'], marker='o', label=device)\n",
    "plt.yscale('log')\n",
    "plt.title('Execution Time Comparison (Logarithmic Scale)')\n",
    "plt.xlabel('Array Size')\n",
    "plt.ylabel('Execution Time (milliseconds)')\n",
    "plt.legend()\n",
    "save_plot_as_image(plt, 'execution_time_comparison_log_scale.png')\n",
    "\n",
    "# Line chart for Throughput\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(5, 1, 3)\n",
    "for device in devices:\n",
    "    device_results = results_df[results_df['Device'] == device]\n",
    "    plt.plot(device_results['Array Size'], device_results['Throughput'], marker='o', label=device)\n",
    "plt.title('Throughput Comparison')\n",
    "plt.xlabel('Array Size')\n",
    "plt.ylabel('Throughput (OPS per second)')\n",
    "plt.legend()\n",
    "save_plot_as_image(plt, 'throughput_comparison.png')\n",
    "\n",
    "# Line chart for Throughput\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(5, 1, 4)\n",
    "for device in devices:\n",
    "    device_results = results_df[results_df['Device'] == device]\n",
    "    plt.plot(device_results['Array Size'], device_results['Throughput'], marker='o', label=device)\n",
    "plt.yscale('log')\n",
    "plt.title('Throughput Comparison (Logarithmic Scale)')\n",
    "plt.xlabel('Array Size')\n",
    "plt.ylabel('Throughput (OPS per second)')\n",
    "plt.legend()\n",
    "save_plot_as_image(plt, 'throughput_comparison_log_scale.png')\n",
    "\n",
    "# Bar chart for Memory Usage\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(5, 1, 5)\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(results_df))\n",
    "memory_cpu = results_df['Memory Usage'][results_df['Device'] == 'cpu']\n",
    "memory_gpu = results_df['Memory Usage'][results_df['Device'] == 'cuda']\n",
    "min_len = min(len(memory_cpu), len(memory_gpu))\n",
    "memory_cpu = memory_cpu[:min_len]\n",
    "memory_gpu = memory_gpu[:min_len]\n",
    "plt.bar(index[:min_len], memory_cpu, bar_width, label='CPU')\n",
    "plt.bar(index[:min_len] + bar_width, memory_gpu, bar_width, label='GPU')\n",
    "plt.title('Memory Usage Comparison')\n",
    "plt.xlabel('Array Size')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.xticks(index[:min_len] + bar_width / 2, results_df['Array Size'][:min_len])\n",
    "plt.legend()\n",
    "save_plot_as_image(plt, 'memory_usage_comparison.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
